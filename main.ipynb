{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "import logging\n",
    "sys.path.append(\"./data\")\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set the device to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                clue             answer\n",
      "0                 Acquisitive chap, as we see it (8)           COVETOUS\n",
      "1             Back yard fencing weak and sagging (6)             DROOPY\n",
      "2  Stripping off uniform, love holding colonel's ...         UNCLOTHING\n",
      "3     Without a mark where they should be gained (4)               EXAM\n",
      "4  Put a stop to Rugby's foul school leader (5,2,...  KNOCK ON THE HEAD\n",
      "5  Foreign letter coming in is the French letter (7)            EPISTLE\n",
      "6          Charge to pack knick-knacks hurriedly (7)            AGITATO\n",
      "7            At first, bear one fruit or another (7)            BANANAS\n",
      "8       Cited tot defending authoritarian leader (7)            ADDUCED\n",
      "9   Heady mixture of qualities nurse developed (7,7)    TEQUILA SUNRISE\n",
      "660613\n",
      "filtered :  658031\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./data/clues.csv\")\n",
    "df.drop(columns=['puzzle_date'])\n",
    "print(df[['clue', 'answer']].head(10))\n",
    "\n",
    "df = df.drop(columns=['puzzle_date'])\n",
    "\n",
    "clues_df = df[df['clue'].notna() & df['answer'].notna()]\n",
    "\n",
    "print(len(df))\n",
    "print(\"filtered : \", len(clues_df))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84609\n"
     ]
    }
   ],
   "source": [
    "#filter for guardian clues. \n",
    "clues_df = clues_df[(clues_df['source'] == 'fifteensquared') & clues_df['source_url'].str.contains('guardian')]\n",
    "print(len(clues_df))\n",
    "\n",
    "clue_answer_tuples = list(zip(clues_df['clue'], clues_df['answer']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample clues dataframe created with rows: 10000\n"
     ]
    }
   ],
   "source": [
    "sample_clues_df = clues_df.head(10000)\n",
    "print(\"Sample clues dataframe created with rows:\", len(sample_clues_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# Load the pretrained RoBERTa model and tokenizer\n",
    "model = RobertaForMaskedLM.from_pretrained('roberta-base')\n",
    "\n",
    "# Load the clues and answers from the CSV file\n",
    "\n",
    "clues = sample_clues_df['clue'].tolist()\n",
    "answers = sample_clues_df['answer'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class ProgressBarCallback(TrainerCallback):\n",
    "    def __init__(self, total_steps, epoch_bar):\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "        self.epoch_bar = epoch_bar\n",
    "        self.batch_bar = None\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        if self.batch_bar is None:\n",
    "            self.batch_bar = tqdm(total=self.total_steps, desc=\"Batch\", leave=False)\n",
    "\n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        self.batch_bar.update(1)\n",
    "        self.current_step += 1\n",
    "\n",
    "        if self.current_step == self.total_steps:\n",
    "            self.batch_bar.close()\n",
    "            self.epoch_bar.update(1)\n",
    "            self.current_step = 0\n",
    "            self.batch_bar = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the dataset\n",
    "class CrypticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, clues, answers, tokenizer, max_length):\n",
    "        self.clues = clues\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        clue = self.clues[idx]\n",
    "        answer = self.answers[idx]\n",
    "\n",
    "        # Tokenize the clue and answer\n",
    "        input_tokens = self.tokenizer.tokenize(clue)\n",
    "        answer_tokens = self.tokenizer.tokenize(answer)\n",
    "\n",
    "        # Mask a portion of the clue (e.g., the definition or wordplay)\n",
    "        masked_clue = self.mask_clue(input_tokens)\n",
    "\n",
    "        # Convert tokens to IDs and add padding\n",
    "        input_ids = self.tokenizer.convert_tokens_to_ids(masked_clue)\n",
    "        input_ids = input_ids[:self.max_length]\n",
    "        input_ids = input_ids + [self.tokenizer.pad_token_id] * (self.max_length - len(input_ids))\n",
    "\n",
    "        answer_ids = self.tokenizer.convert_tokens_to_ids(answer_tokens)\n",
    "        answer_ids = answer_ids[:self.max_length]\n",
    "        answer_ids = answer_ids + [self.tokenizer.pad_token_id] * (self.max_length - len(answer_ids))\n",
    "\n",
    "        # Create attention masks -> pay attention to clue, not padding\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        attention_mask = attention_mask + [0] * (self.max_length - len(attention_mask))\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'attention_mask': torch.tensor(attention_mask),\n",
    "            'labels': torch.tensor(answer_ids)\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.clues)\n",
    "    \n",
    "    def mask_clue(self, tokens):\n",
    "        # Implement logic to mask different parts of the clue\n",
    "        # For example, mask the last word (often the definition)\n",
    "        tokens[-1] = '<mask>'\n",
    "        return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:   0%|          | 0/5 [01:44<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total steps per epoch is 625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the dataset\n",
    "max_length = 128\n",
    "dataset = CrypticDataset(clues, answers, tokenizer, max_length)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Create the progress bar for epochs\n",
    "num_epochs = training_args.num_train_epochs\n",
    "\n",
    "epoch_bar = tqdm(total=num_epochs, desc=\"Epoch\")\n",
    "total_steps_per_epoch = len(dataset) // training_args.per_device_train_batch_size\n",
    "print(\"total steps per epoch is\", total_steps_per_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "625\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "print(10000 // training_args.per_device_train_batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aj\\Code\\Cryptic_project\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "trainer = Trainer(\n",
    "    model=model.to(device),\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 18:08:31,461 - INFO - Epoch: 1/5\n",
      "2024-05-14 18:12:56,985 - INFO - Batch: 100/625\n",
      "2024-05-14 18:16:37,534 - INFO - Batch: 200/625\n",
      "2024-05-14 18:20:23,110 - INFO - Batch: 300/625\n",
      "2024-05-14 18:23:55,648 - INFO - Batch: 400/625\n",
      "2024-05-14 18:27:29,385 - INFO - Batch: 500/625\n",
      "2024-05-14 18:31:01,117 - INFO - Batch: 600/625\n",
      "2024-05-14 18:31:54,193 - INFO - Epoch 1 completed.\n",
      "2024-05-14 18:31:54,194 - INFO - Epoch: 2/5\n",
      "2024-05-14 18:35:25,367 - INFO - Batch: 100/625\n",
      "2024-05-14 18:38:52,342 - INFO - Batch: 200/625\n",
      "2024-05-14 18:42:09,553 - INFO - Batch: 300/625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainer\u001b[38;5;241m.\u001b[39mget_train_dataloader()):\n\u001b[0;32m      6\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     10\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_steps_per_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\aj\\Code\\Cryptic_project\\Lib\\site-packages\\transformers\\trainer.py:3045\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   3043\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3044\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3045\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3047\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[1;32mc:\\Users\\aj\\Code\\Cryptic_project\\Lib\\site-packages\\accelerate\\accelerator.py:2001\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   1999\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2000\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2001\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aj\\Code\\Cryptic_project\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aj\\Code\\Cryptic_project\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fine-tune the model with logging\n",
    "for epoch in range(num_epochs):\n",
    "    logger.info(f\"Epoch: {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    for step, inputs in enumerate(trainer.get_train_dataloader()):\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        trainer.training_step(model, inputs)\n",
    "        \n",
    "        if (step + 1) % 100 == 0:\n",
    "            logger.info(f\"Batch: {step+1}/{total_steps_per_epoch}\")\n",
    "    \n",
    "    logger.info(f\"Epoch {epoch+1} completed.\")\n",
    "    model.save_pretrained(f'./fine_tuned_model_epoch_{epoch+1}')\n",
    "    \n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained('./fine_tuned_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_clues_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cryptic_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
